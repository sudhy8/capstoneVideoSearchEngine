{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sudhy\\.conda\\envs\\capstoneProject\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# from transformers import CLIPProcessor, CLIPModel,PreTrainedTokenizerFast\n",
    "from transformers import PreTrainedTokenizerFast, CLIPProcessor, CLIPModel,CLIPTokenizerFast\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import weaviate\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \\\n",
    "(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \\\n",
    "(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "model = CLIPModel. from_pretrained(model_id) . to(device)\n",
    "tokenizer = CLIPTokenizerFast. from_pretrained(model_id)\n",
    "processor = CLIPProcessor. from_pretrained(model_id)\n",
    "\n",
    "\n",
    "with weaviate.connect_to_wcs(\n",
    "    cluster_url=os.getenv(\"WEAVIATE_CLUSTER_URL\", \"https://ccc-q2m6s9m3.weaviate.network\"),  # Replace with your WCS URL\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\", \"imlIdfNFZ8eC6rm4iCvSrTJyQeoBo6KjQtLT\"))  # Replace with your WCS key\n",
    ") as client:  # Use this context manager to ensure the connection is closed\n",
    "    print(client.is_ready())\n",
    "\n",
    "client.connect()\n",
    "images =[Image.open('./deer.jpg'),Image.open('./hill.jpg'),Image.open('./girl.jpg'),Image.open('./elder.jpg'),Image.open('./elder2.jpg')]\n",
    "names =['deer.jpg','hill.jpg','girl.jpg','elder.jpg','elder2.jpg']\n",
    "\n",
    "def img_embedder(images,names):\n",
    "    batch_size = 16\n",
    "    image_arr = None\n",
    "\n",
    "    for i in tqdm(range(0, len(images), batch_size)):\n",
    "        # select batch of images\n",
    "        batch = images[i:i+batch_size]\n",
    "        # process and resize\n",
    "        batch = processor(\n",
    "            text=None,\n",
    "            images=batch,\n",
    "            return_tensors='pt',\n",
    "            padding=True\n",
    "        )['pixel_values'].to(device)\n",
    "        # get image embeddings\n",
    "        batch_emb = model.get_image_features(pixel_values=batch)\n",
    "        # convert to numpy array\n",
    "        batch_emb = batch_emb.squeeze(0)\n",
    "        batch_emb = batch_emb.cpu().detach().numpy()\n",
    "        # add to larger array of all image embeddings\n",
    "        if image_arr is None:\n",
    "            image_arr = batch_emb\n",
    "        else:\n",
    "            image_arr = np.concatenate((image_arr, batch_emb), axis=0)\n",
    "    image_arr.shape\n",
    "\n",
    "\n",
    "    videoSearch = client.collections.get(\"VideoSearch\")\n",
    "    for i in zip(image_arr,names):\n",
    "        uuid =videoSearch.data.insert(\n",
    "            {\n",
    "            \"name\": i[1]\n",
    "        },\n",
    "        vector = i[0].tolist()\n",
    "        )\n",
    "        print(uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstoneProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
